# Subset dataset
BC.onroad <- subset(NEI, fips == '24510' & type == 'ON-ROAD')
CA.onroad <- subset(NEI, fips == '06037' & type == 'ON-ROAD')
# Aggregate
BC.DF <- aggregate(BC.onroad[, 'Emissions'], by=list(BC.onroad$year), sum)
colnames(BC.DF) <- c('year', 'Emissions')
BC.DF$City <- paste(rep('BC', 4))
CA.DF <- aggregate(CA.onroad[, 'Emissions'], by=list(CA.onroad$year), sum)
colnames(CA.DF) <- c('year', 'Emissions')
CA.DF$City <- paste(rep('CA', 4))
DF <- as.data.frame(rbind(BC.DF, CA.DF))
# Plot graph
png('plot6.png')
ggplot(data=DF, aes(x=year, y=Emissions)) + geom_bar(stat="identity", aes(fill=year)) + guides(fill=F) +
ggtitle('Total Emissions of Motor Vehicle Sources\nLos Angeles County, California vs. Baltimore City, Maryland') +
ylab(expression('PM'[2.5])) + xlab('Year') + theme(legend.position='none') + facet_grid(. ~ City) +
geom_text(aes(label=round(Emissions,0), size=1, hjust=0.5, vjust=-1))
dev.off()
rm(list=ls())
# copy the data into your working directory
# Load provided data
data=read.csv("activity.csv",stringsAsFactors = FALSE)
# Change class for the date variable
data[[2]]=as.Date(data[[2]])
# copy the data into your working directory
# Load provided data
data=read.csv("activity.csv",stringsAsFactors = FALSE)
# Change class for the date variable
data[[2]]=as.Date(data[[2]])
data
sum<- aggregate(steps~date, FUN=sum, data=data , na.rm=TRUE)
m=mean(sum[[2]])
me=median(sum[[2]])
plot(sum[[1]],sum[[2]],type="h",main="Number of steps taken daily", xlab= "Date",ylab="No. of steps taken")
activity <- read.csv("Data/activity.csv")
activity <- read.csv("activity.csv")
activity$date <- as.Date(activity$date,"%Y-%m-%d")
activityClean <- na.omit(activity)
totalSteps<-aggregate(steps~date,data=activityClean,sum)
totalSteps <- aggregate(steps~date,data=activityClean,sum)
aggreagate
aggregate
activity <- read.csv(".\\activity.csv")
stepsum <- tapply(activity$steps,activity$date,sum, na.rm=TRUE)
par(mar = c(6, 4, 2, 1))
barplot(stepsum, main="Total number of steps taken per day", las=2, col="blue")
stepsum <- tapply(activity$steps,activity$date,sum, na.rm=TRUE)
par(mar = c(6, 4, 2, 1))
barplot(stepsum, main="Total number of steps taken per day", las=2, col="blue")
library(ggplot2)
library(ggplot2)
library(plyr)
library(reshape2)
opts_chunk$set(message = FALSE)
library(knitr)
opts_chunk$set(message = FALSE)
data$date <- as.Date(data$date, format="%Y-%m-%d")
data1 <- na.omit(data)
steps <- tapply(data1$steps,factor(data1$date),FUN = sum)
qplot(steps,
main="Histogram of total number of steps per day",
xlab="Total number of steps in a day")
require(knitr)
opts_chunk$set(message = FALSE)
library(ggplot2)
library(ggplot2)
library(plyr)
library(reshape2)
unzip(zipfile="activity.zip")
data <- read.csv("activity.csv",stringsAsFactors=F)
data$date <- as.Date(data$date, format="%Y-%m-%d")
data1 <- na.omit(data)
steps <- tapply(data1$steps,factor(data1$date),FUN = sum)
qplot(steps,
main="Histogram of total number of steps per day",
xlab="Total number of steps in a day")
data.m <- melt(data,id=c("interval","date"),na.rm = T)
avgs <- dcast(data.m,interval~variable,mean)
ggplot(data=avgs,aes(x=interval,y=steps)) +
geom_line() +
xlab("5-minute interval") +
ylab("average number of steps taken")
avgs[which.max(avgs$steps),]$interval
#the impute function returns the mean value of its 5-minute interval
impute <- function(interval, steps) {
imputed <- NA
if(!is.na(steps))
imputed <- steps
else
imputed <- avgs[avgs$interval==interval,"steps"]
return(imputed)
}
imputed_data <- data
imputed_data$steps <- mapply(impute,imputed_data$interval,imputed_data$steps)
steps <- tapply(imputed_data$steps, factor(imputed_data$date), FUN=sum)
qplot(steps, xlab="total number of steps taken each day")
imputed_data$Weekday <- as.factor(ifelse(weekdays(imputed_data$date) %in% c("Saturday","Sunday"),"weekend","weekday"))
# date variable is not useful in the analysis
melted <- melt(imputed_data[,-2],id=c("interval","Weekday"))
avgs <- dcast(melted,Weekday+interval~variable,mean)
ggplot(avgs, aes(interval, steps)) + geom_line() + facet_grid(Weekday ~ .) +
xlab("5-minute interval") + ylab("Number of steps")
rm(list=ls())
data=read.csv("activity.csv",stringsAsFactors = FALSE)
# Change class for the date variable
data[[2]]=as.Date(data[[2]])
sum<- aggregate(steps~date, FUN=sum, data=data , na.rm=TRUE)
m=mean(sum[[2]])
me=median(sum[[2]])
plot(sum[[1]],sum[[2]],type="h",main="Number of steps taken daily", xlab= "Date",ylab="No. of steps taken")
#Summarize the data by interval
mean_interval<-aggregate(steps~interval, FUN="mean",data=data, na.rm=TRUE)
#maximum value calculation
x=mean_interval[mean_interval[[2]]==max(mean_interval[[2]]),]
plot(mean_interval[[1]],mean_interval[[2]],type="l",xlab="Interval", ylab = "Average steps per interval",main=" Average of steps taken for each interval")
## unique(is.na(data[[3]]))
## unique(is.na(data[[2]]))
## by exe. above statements - resul = FALSE for both the cases
## while unique(is.na(data[[1]])) results in both TRUE and FALSE
## N.A only in the first column
# Use previously calculated means
mean_interval$imputed_steps <- floor(mean_interval$steps)
# Merge the replacement values
mean_interval <- merge(data,mean_interval[,c('interval', 'imputed_steps')],
by='interval')
for(i in 1:nrow(mean_interval))
{
if(is.na(mean_interval[i,2]))
{
mean_interval[i,2]=mean_interval[i,4]
}
}
# Remove unnecesary data
mean_interval$imputed_steps <- NULL
sum2<- aggregate(steps~date, FUN="sum",data=mean_interval)
m2=mean(sum2[[2]])
me2=median(sum2[[2]])
##diving the data into weekdays and weekend
wk_days=data[weekdays(data[[2]])=="Sunday"|weekdays(data[[2]])=="Saturday",]
wk_ends=data[weekdays(data[[2]])=="Monday"|weekdays(data[[2]])=="Tuesday"|weekdays(data[[2]])=="Wednesday"|weekdays(data[[2]])=="Thursday"|weekdays(data[[2]])=="Friday",]
#summarise each dataframe- accd. to interval
wd_mean<- aggregate(steps~interval, FUN="mean",data=wk_days)
we_mean<- aggregate(steps~interval, FUN="mean",data=wk_ends)
par(mfrow=c(2,1))
plot(wd_mean[[1]],wd_mean[[2]],xlab="Interval", ylab = "Average steps per interval",main=" Average of steps taken for each interval-weekdays",type="l")
plot(we_mean[[1]],we_mean[[2]],xlab="Interval", ylab = "Average steps per interval",main=" Average of steps taken for each interval-weekends",type="l")
##diving the data into weekdays and weekend
wk_days=data[weekdays(data[[2]])=="Sunday"|weekdays(data[[2]])=="Saturday",]
wk_ends=data[weekdays(data[[2]])=="Monday"|weekdays(data[[2]])=="Tuesday"|weekdays(data[[2]])=="Wednesday"|weekdays(data[[2]])=="Thursday"|weekdays(data[[2]])=="Friday",]
#summarise each dataframe- accd. to interval
wd_mean<- aggregate(steps~interval, FUN="mean",data=wk_days)
we_mean<- aggregate(steps~interval, FUN="mean",data=wk_ends)
par(mfrow=c(2,1))
plot(wd_mean[[1]],wd_mean[[2]],xlab="Interval", ylab = "Average steps per interval",main=" Average of steps taken for each interval-weekdays",type="l")
plot(we_mean[[1]],we_mean[[2]],xlab="Interval", ylab = "Average steps per interval",main=" Average of steps taken for each interval-weekends",type="l")
rm(list=ls())
activity <- read.csv("Data/activity.csv")
activity <- read.csv("activity.csv")
activity$date <- as.Date(activity$date,"%Y-%m-%d")
activityClean <- na.omit(activity)
totalSteps<-aggregate(steps~date,data=activityClean,sum)
hist(totalSteps$steps,col="red",
xlab="Total Steps Taken Each Day",ylab="Frequency",breaks=10,
main="Histogram of Total Steps Taken Each Day")
hist(totalSteps$steps,col="red",
xlab="Total Steps Taken Each Day",ylab="Frequency",breaks=10,
main="Histogram of Total Steps Taken Each Day")
meanSteps<-aggregate(steps~interval,data=activityClean,mean,na.rm=TRUE)
plot(steps~interval,data=meanSteps,type="l",
xlab="Time Intervals (5-minute)",
ylab="Mean Number of Steps Taken",
main="Average Number of Steps Taken Across All Days",
col="red")
missingSteps <- numeric()
for(i in 1:nrow(activity)) {
rd <- activity[i,]
if (is.na(rd$steps)) {
steps <- subset(meanSteps,interval==rd$interval)$steps
} else {
steps <- rd$steps
}
missingSteps <- c(missingSteps,steps)
}
activityMissingSteps <- activity
activityMissingSteps$steps <- missingSteps
imp_steps_per_day <- aggregate(steps~date, data=activityMissingSteps,sum)
hist(imp_steps_per_day$steps,
main="Total Number of Steps/Day With Missign Values",breaks=10,
xlab="Steps/Day", col="blue")
par(mfrow=c(1,3))
hist(totalSteps$steps,col=rgb(0,0,1,0.5),breaks=10,
xlab="Total Steps Taken Each Day",ylab="Frequency",
main="Data w/o missing values")
hist(imp_steps_per_day$steps, col=rgb(1,0,0,0.5),breaks=10,
xlab="Total Steps Taken Each Day",ylab="Frequency",
main="Data w/ missing values imputed")
hist(totalSteps$steps,col=rgb(0,0,1,0.5),breaks=10,
xlab="Total Steps Taken Each Day",ylab="Frequency",
main="Overlapping Histogram")
hist(imp_steps_per_day$steps, col=rgb(1,0,0,0.5),breaks=10, add=T)
box()
meanTotalSteps<-aggregate(steps~interval,data=activityMissingSteps,mean,na.rm=TRUE)
par(mfrow=c(1,1))
plot(steps~interval,data=meanSteps,type="l",
xlab="Time Intervals (5-minute)",
ylab="Mean Number of Steps Taken",
main="Average Number of Steps Taken Across All Days (overlapped)",
col=rgb(0,0,1,0.5))
par(new=T)
plot(steps~interval,data=meanTotalSteps,type="l",
col=rgb(1,0,0,0.5))
Day <- weekdays(activityMissingSteps$date)
activityMissingSteps$day_type <- ifelse(Day == "sabado" | Day == "domingo","Weekend", "Weekday")
meanSteps.imputed <- aggregate(activityMissingSteps$steps,
by=list(activityMissingSteps$interval,activityMissingSteps$day_type),mean)
names(meanSteps.imputed) <- c("interval","day_type","steps")
library(lattice)
xyplot(steps~interval | day_type, meanSteps.imputed,type="l",layout=c(1,2),
xlab="Interval",ylab = "Number of steps",
main="Activity Patterns on Weekends and Weekdays",col = "blue")
meanSteps.imputed <- aggregate(activityMissingSteps$steps,
by=list(activityMissingSteps$interval,activityMissingSteps$day_type),mean)
names(meanSteps.imputed) <- c("interval","day_type","steps")
library(lattice)
xyplot(steps~interval | day_type, meanSteps.imputed,type="l",layout=c(1,2),
xlab="Interval",ylab = "Number of steps",
main="Activity Patterns on Weekends and Weekdays",col = "blue")
rm(list=ls())
library(ggplot2)
CSV_FILE <- "activity.csv"
ZIP_FILE <- "activity.zip"
if(!file.exists(CSV_FILE)) {
unzip(ZIP_FILE)
}
act_data <- read.csv(CSV_FILE)
act_data$date <- as.Date(act_data$date, "%Y-%m-%d")
act_data_orig <- act_data
SATURDAY <- 6
SUNDAY <- 0
options(scipen = 1, digits = 2)
agg_data_sum_day <- aggregate(x = act_data$steps, by = list(date = act_data$date), FUN = "sum", na.rm=TRUE)
colnames(agg_data_sum_day) <- c("date", "steps")
suppressMessages(print(qplot(x=agg_data_sum_day$steps, geom = "histogram", xlab="Total no. of steps", ylab="Frequency")))
act_pattern <- act_data[,c(1,3)] # Remove 'date' column
act_pattern <- aggregate(x = act_data$steps, by = list(interval = act_data$interval), FUN = "mean", na.rm=TRUE)
print(qplot(x=act_pattern$interval, y=act_pattern$x, data=act_pattern, geom="line",
ylab="Steps Average", xlab="Interval"))
act_data_modif <- act_data
for(row in which(is.na(act_data_modif$steps))){
gen_value <-
ceiling(
mean(
act_data[which(act_data$interval == act_data_modif[row,]$interval),]$steps, na.rm=TRUE)) + 1
act_data_modif[row,]$steps <- gen_value
}
act_data <- act_data_modif # replace original data, so chunk 'histogram_total_steps' can be re-used below
agg_data_sum_day <- aggregate(x = act_data$steps, by = list(date = act_data$date), FUN = "sum", na.rm=TRUE)
colnames(agg_data_sum_day) <- c("date", "steps")
suppressMessages(print(qplot(x=agg_data_sum_day$steps, geom = "histogram", xlab="Total no. of steps", ylab="Frequency")))
act_data_week <- act_data_orig # get non-imputed data
weekends <- which(as.POSIXlt(act_data_week$date)$wday %in% c(SATURDAY,SUNDAY))
act_data_week["type_of_day"] <- "weekday"
act_data_week[weekends,]$type_of_day <- "weekend"
act_data_week$type_of_day <- factor(act_data_week$type_of_day)
act_data_week <- aggregate(x = act_data_week$steps,
by = list(interval = act_data_week$interval,
type_of_day = act_data_week$type_of_day),
FUN = "sum", na.rm=TRUE)
colnames(act_data_week) <- c("interval", "type_of_day", "steps")
p <- ggplot(data=act_data_week, aes(x=interval, y=steps)) + geom_line(shape=1)
p <- p + facet_grid(type_of_day ~ .)
print(p)
rm(list=ls())
opts_chunk$set(message = FALSE)
require('ggplot2')
if(!file.exists('./data')){dir.create('./data')}
fileUrl <- 'https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2Factivity.zip'
destfile <- 'repdata_data_activity.zip'
download.file(fileUrl, destfile=paste('data', destfile, sep='/'))
## Unzip the dataset
unzip(paste('data', destfile, sep='/'), exdir='data')
data_dir <- setdiff(dir('data'), destfile)
## Read the dataset
activity <- read.csv('./data/activity.csv', header=T, sep=',', na.strings='?', nrows=17568)
activity$steps <- as.numeric(activity$steps)
activity$date <- as.Date(activity$date)
activity$interval <- as.numeric(activity$interval)
activity <- read.csv('./data/activity.csv', header=T, sep=',', na.strings='?', nrows=17568)
activity$steps <- as.numeric(activity$steps)
activity$date <- as.Date(activity$date)
activity$interval <- as.numeric(activity$interval)
steps.date <- aggregate(steps ~ date, data=activity, FUN=sum, na.rm=TRUE)
barplot(steps.date$steps, names.arg=steps.date$date, xlab="date", ylab="steps")
mean(steps.date$steps)
median(steps.date$steps)
steps.interval <- aggregate(steps ~ interval, data=activity, FUN=mean)
plot(steps.interval, type="l")
steps.interval$interval[which.max(steps.interval$steps)]
sum(is.na(activity))
activity <- merge(activity, steps.interval, by="interval", suffixes=c("",".y"))
nas <- is.na(activity$steps)
activity$steps[nas] <- activity$steps.y[nas]
activity <- activity[,c(1:3)]
steps.date <- aggregate(steps ~ date, data=activity, FUN=sum)
barplot(steps.date$steps, names.arg=steps.date$date, xlab="date", ylab="steps")
mean(steps.date$steps)
median(steps.date$steps)
daytype <- function(date) {
if (weekdays(as.Date(date)) %in% c("Saturday", "Sunday")) {
"weekend"
} else {"weekday"}}
activity$daytype <- as.factor(sapply(activity$date, daytype))
par(mfrow=c(2,1))
for (type in c("weekend", "weekday")) {
steps.type <- aggregate(steps ~ interval,
data=activity,
subset=activity$daytype==type,
FUN=mean)
plot(steps.type, type="l", main=type)
}
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
steps.date <- aggregate(steps ~ date, data=activity, FUN=sum, na.rm=TRUE)
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
knit()
knit
require(knitr)
knit
library(knitr)
library(markdown)
knit("PA1_template.Rmd")
install.packages(c("car", "dendextend", "googleVis", "NLP", "RCurl", "rjson", "RJSONIO", "xlsx"))
require('dev')
#Only the first time data is download, a "data" directory is created.
if (!file.exists("data")) {
dir.create("data")
}
#and stores that file into "data" directory
if(!file.exists("data/repdata-data-StormData.csv.bz2")) {
fileURL <- "http://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2"
download.file(fileURL, destfile = "./data/repdata-data-StormData.csv.bz2")
}
#Uncompress and reads if not exists. Stores data in storm.noaa.data variable.
if(!file.exists("data/storm_noaa_data.dat")){
storm.noaa.data <- read.csv(bzfile("./data/repdata-data-StormData.csv.bz2"))
write.table(storm.noaa.data, "data/storm_noaa_data.dat")
}else{
storm.noaa.data <- read.table("data/storm_noaa_data.dat")
}
#subsets those requested columns and stores in work.data variable.
work.data <- subset(storm.noaa.data, FATALITIES > 0 | INJURIES > 0 | PROPDMG > 0 | CROPDMG > 0, select = c(8, 23:28))
#transforms exponents into numeric values hecto=100, kilo=1000, million=1000000=1e+06 etc..
expcalc <- function(dam,exp) {
if (exp == "") {
dam * 1
} else if (exp == "1") {
dam * 10
} else if (exp == "H" | exp == "h") {
dam * 100
} else if (exp == "K" | exp == "k") {
dam * 1000
} else if (exp == "M" | exp == "m") {
dam * 1e+06
} else if (exp == "B" | exp == "b") {
dam * 1e+09
} else 0
}
#creates two new columns,applying those exponents to property damage(PDCOST), and crop damage(CROPCOST)
work.data$PDCOST <- mapply(expcalc, work.data$PROPDMG, work.data$PROPDMGEXP)
work.data$CROPCOST <- mapply(expcalc, work.data$CROPDMG, work.data$CROPDMGEXP)
##trying to get tidier data, transform EVTYPE column to lower case. Makes it as factor again.
work.data$EVTYPE <- tolower(work.data$EVTYPE)
work.data$EVTYPE <- as.factor(work.data$EVTYPE)
require(ggplot2)
require(reshape2)
# melt function needs reshape2 package
#melt function: id.vars is a vector of id variables. measure.vars is a vector of measured variables
#In pop.health stores 3 columns: "EVTYPE"   "variable" "value"
pop.health <- melt(work.data[,c(1:3)], id.vars = "EVTYPE", measure.vars= c("FATALITIES", "INJURIES"))
#In pop.health.agg stores 3 columns: "EVTYPE"        "Casualty" "Count"
#there are 488 events, but 2 types: fatalities and injuries, so 976 rows
pop.health.agg <- setNames(aggregate(value ~ EVTYPE+variable, data=pop.health, sum, na.rm = TRUE), c("EVTYPE","Casualty","Count"))
#In total stores each 447 type of events and the sum/Count each one
total <- aggregate(Count ~ EVTYPE, pop.health.agg, sum)
#In pop.health.agg2 stores "EVTYPE" "Casualty" "Count.x" "Count.y"
pop.health.agg2 <- merge(pop.health.agg, total, by = "EVTYPE")
#orders and select first 20
pop.health.agg2 <- pop.health.agg2[order(-pop.health.agg2$Count.y, pop.health.agg2$Casualty), ][1:20, ]
#Prepares by factoring to plot
pop.health.agg2$EVTYPE <-suppressWarnings(factor(pop.health.agg2$EVTYPE, levels=pop.health.agg2[order(pop.health.agg2$Count.y), "EVTYPE"]))
ggplot(pop.health.agg2, aes(x = EVTYPE, y = Count.x, fill = Casualty)) + geom_bar(stat = "identity") + coord_flip() +
ylab("Fatality Injury") +
xlab("Type of Event") +
ggtitle("Fatalities and Injuries by Type of Event")
storm <- read.csv("storm_data.csv.bz2")
str(storm)
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.packages('caret')
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.packages('AppliedPredictiveModeling')
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
require('shiny')
install.packages(c("bdsmatrix", "BH", "ChainLadder", "DBI", "FactoMineR", "formatR", "gsubfn", "httr", "labeling", "markdown", "NLP", "packrat", "RCurl", "rjson", "RJSONIO", "sandwich", "spacetime", "swirl", "xlsx", "xlsxjars"))
install.packages(c("bdsmatrix", "ChainLadder", "DBI", "FactoMineR", "formatR", "gsubfn", "httr", "labeling", "markdown", "NLP", "packrat", "RCurl", "rjson", "RJSONIO", "sandwich", "spacetime", "swirl", "xlsx", "xlsxjars"))
install.packages(c("bdsmatrix", "RCurl", "rjson", "RJSONIO", "xlsx", "xlsxjars"))
install.packages('bdsmatrix')
install.packages("bdsmatrix")
install.packages("bdsmatrix")
install.packages("bdsmatrix")
install.packages(c("bdsmatrix", "RCurl", "rjson", "RJSONIO", "xlsx", "xlsxjars"))
require('shiny')
require('kntri')
require('knitr')
getwd()
mydir <- paste0('getwd(),C:/Users/Scibrokes Trading/Documents/GitHub/englianhu/RepData_PeerAssessment2')
setwd <- mydir
library(knitr)
library(markdown)
# knitr configuration
opts_knit$set(progress=FALSE)
opts_chunk$set(echo=TRUE, message=FALSE, tidy=TRUE, comment=NA,
fig.path="figure/", fig.keep="high", fig.width=10, fig.height=6,
fig.align="center")
knit2html('stormDataAnalysis.Rmd')
browseURL('stormDataAnalysis.html')
mydir <- paste0('getwd(),C:/Users/Scibrokes Trading/Documents/GitHub/englianhu/RepData_PeerAssessment1')
setwd <- mydir
library(knitr)
library(markdown)
# knitr configuration
opts_knit$set(progress=FALSE)
opts_chunk$set(echo=TRUE, message=FALSE, tidy=TRUE, comment=NA,
fig.path="figure/", fig.keep="high", fig.width=10, fig.height=6,
fig.align="center")
knit2html('PA1_template.Rmd')
browseURL('PA1_template.html')
mydir
mydir <- paste0(getwd(),'C:/Users/Scibrokes Trading/Documents/GitHub/englianhu/RepData_PeerAssessment1')
setwd <- mydir
library(knitr)
library(markdown)
# knitr configuration
opts_knit$set(progress=FALSE)
opts_chunk$set(echo=TRUE, message=FALSE, tidy=TRUE, comment=NA,
fig.path="figure/", fig.keep="high", fig.width=10, fig.height=6,
fig.align="center")
knit2html('PA1_template.Rmd')
browseURL('PA1_template.html')
mydir
install.packages("rpubchem")
require('rpubchem')
require(markdown)
library(knitr)
library(markdown)
library(Hmisc)
library(reshape)
library(ggplot2)
library(car)
library(knitr)
library(markdown)
library(formatR)
library(Hmisc)
library(reshape)
library(ggplot2)
library(car)
# knitr configuration
opts_knit$set(progress=FALSE)
opts_chunk$set(echo=TRUE, message=FALSE, tidy=TRUE, comment=NA,
fig.path="figure/", fig.keep="high", fig.width=10, fig.height=6,
fig.align="center")
rpubsUpload(title, htmlFile, id = NULL, properties = list(),
method = getOption("rpubs.upload.method", "internal"))
knit2html('stormDataAnalysis.Rmd')
browseURL('stormDataAnalysis.html')
rpubsUpload(title, htmlFile, id = NULL, properties = list(),
method = getOption("rpubs.upload.method", "internal"))
ToothGrowth
mydir <- paste0(getwd(),'/GitHub/englianhu/RepData_PeerAssessment2')
setwd(mydir)
library(knitr)
library(markdown)
library(formatR)
library(Hmisc)
library(reshape)
library(ggplot2)
library(car)
# knitr configuration
opts_knit$set(progress=FALSE)
opts_chunk$set(echo=TRUE, message=FALSE, tidy=TRUE, comment=NA,
fig.path="figure/", fig.keep="high", fig.width=10, fig.height=6,
fig.align="center")
knit2html('stormDataAnalysis.Rmd')
browseURL('stormDataAnalysis.html')
# rpubsUpload(title, htmlFile, id = NULL, properties = list(),
#             method = getOption("rpubs.upload.method", "internal"))
require('swirl')
swirl()
library(datasets)
ToothGrowth
mtcars
require(swirl)
install_from_swirl('Regression Models')
swirl()
swirl()
swirl()
swirl()
0
